include(joinpath(@__DIR__, "..", "src", "preprocessing", "CharProcessing.jl"))


@testset "tokenize_char" begin
    txt = "CafÃ© ğŸ˜Š"

    # default: NFC, no spaces, original case
    toks = tokenize_char(txt)
    @test toks == ["C","a","f","Ã©","ğŸ˜Š"]

    # lower-case, keep space
    toks2 = tokenize_char(txt; lower = true, keep_space = true)
    @test toks2 == ["c","a","f","Ã©"," ","ğŸ˜Š"]

    # ensure combined accents survive when we skip normalisation (:NFD)
    nfd = Unicode.normalize("Ã©", :NFD)  # 'e' + combining acute
    toks3 = tokenize_char(nfd; normalize = false)
    @test length(toks3) == 2            # two graphemes without NFC merge
end

@testset "chars_to_ids" begin
    voc = Vocabulary(Dict("<unk>" => 1), ["<unk>"], Dict{Int,Int}(), 1)
    ids = chars_to_ids(["a","b","a"], voc; add_new = true)

    @test ids == [2,3,2]                   # â€œaâ€ got id 2, â€œbâ€ id 3
    @test voc.id2token[2:3] == ["a","b"]   # vocabulary grew as expected
    @test chars_to_ids(["z"], voc) == [1]  # unk when add_new = false
end


@testset "encode_char_batch" begin
    voc = Vocabulary(Dict("<unk>" => 1, "h"=>2, "i"=>3, "ğŸ˜Š"=>4),
                     ["<unk>","h","i","ğŸ˜Š"],
                     Dict{Int,Int}(), 1)

    mat = encode_char_batch(["hi","ğŸ˜Šh"], voc)
    @test size(mat) == (2, 2)              # longest seq = 2
    @test mat[:,1] == [2,3]                # â€œh iâ€
    @test mat[1,2] == 4 && mat[2,2] == 2   # â€œğŸ˜Š hâ€ (padded already OK)
end